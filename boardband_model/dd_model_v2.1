import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType};
import org.apache.spark.{SparkConf, SparkContext, SparkException};
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.mllib.regression.LinearRegressionWithSGD;
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD;
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS;
import org.apache.spark.mllib.tree.DecisionTree;
import org.apache.spark.mllib.tree.model.DecisionTreeModel;

import org.apache.spark.mllib.regression.LabeledPoint;
import org.apache.spark.mllib.linalg.Vectors;
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics;
import org.apache.spark.mllib.util.MLUtils;



val conf = new SparkConf();

@transient val hiveCtx = new HiveContext(sc);

hiveCtx.sql(s"use jhzx_hive_db");


val pre_Data=hiveCtx.sql("select use_num,ARPU,KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,SEARCH_BJ_FLAG,SEARCH_ZJ_FLAG,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,cluster2,cluster3,if(ARPU<50,1,0) low_arpu,if(ARPU>=50 and ARPU<150,1,0) mid_arpu,if(ARPU>=150,1,0) high_arpu,if(use_num<500,1,0) low_gprs,if(use_num>=500 and use_num<2000,1,0) mid_gprs,if(use_num>=2000,1,0) high_gprs,stay_flag,LABEL from jcc_jh_dataset_15 ");

val pre_Data=hiveCtx.sql("select use_num,KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,SEARCH_BJ_FLAG,SEARCH_ZJ_FLAG,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,upay_flag,cluster2,cluster3,ARPU,ownserv_flag,otherserv_flag,stay_flag,LABEL from jcc_jh_dataset_15 ");

val data1=pre_Data.rdd.map(x =>(x(0).toString.toDouble, x(1).toString.toDouble,x(2).toString.toDouble,x(3).toString.toDouble,x(4).toString.toDouble, x(5).toString.toDouble,x(6).toString.toDouble,x(7).toString.toDouble,x(8).toString.toDouble,x(9).toString.toDouble,x(10).toString.toDouble,x(11).toString.toDouble,x(12).toString.toDouble,x(13).toString.toDouble,x(14).toString.toDouble,x(15).toString.toDouble,x(16).toString.toDouble,x(17).toString.toDouble,x(18).toString.toDouble,x(19).toString.toDouble,x(20).toString.toDouble));

val data2=data1.map(x=>LabeledPoint(x._21,Vectors.dense(x._1,x._2,x._3,x._4,x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14,x._15,x._16,x._17,x._18,x._19,x._20)));


val pre_Data=hiveCtx.sql("select user_id,KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,SEARCH_BJ_FLAG,SEARCH_ZJ_FLAG,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,cluster2,cluster3,if(ARPU<50,1,0) low_arpu,if(ARPU>=50 and ARPU<150,1,0) mid_arpu,if(ARPU>=150,1,0) high_arpu,if(use_num<500,1,0) low_gprs,if(use_num>=500 and use_num<2000,1,0) mid_gprs,if(use_num>=2000,1,0) high_gprs,LABEL from jcc_jh_dataset_15 ");

//去掉stay_flag 0.4
val pre_Data=hiveCtx.sql("select KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,SEARCH_BJ_FLAG,SEARCH_ZJ_FLAG,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,cluster2,cluster3,if(ARPU<50,1,0) low_arpu,if(ARPU>=50 and ARPU<150,1,0) mid_arpu,if(ARPU>=150,1,0) high_arpu,if(use_num<500,1,0) low_gprs,if(use_num>=500 and use_num<2000,1,0) mid_gprs,if(use_num>=2000,1,0) high_gprs,LABEL from jcc_jh_dataset_15 ");

val data1=pre_Data.rdd.map(x =>(x(0).toString.toDouble, x(1).toString.toDouble,x(2).toString.toDouble,x(3).toString.toDouble,x(4).toString.toDouble, x(5).toString.toDouble,x(6).toString.toDouble,x(7).toString.toDouble,x(8).toString.toDouble,x(9).toString.toDouble,x(10).toString.toDouble,x(11).toString.toDouble,x(12).toString.toDouble,x(13).toString.toDouble,x(14).toString.toDouble,x(15).toString.toDouble,x(16).toString.toDouble,x(17).toString.toDouble));

val data2=data1.map(x=>LabeledPoint(x._18,Vectors.dense(x._1,x._2,x._3,x._4,x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14,x._15,x._16,x._17)));


//增加new_user_flag RG 0.63,0.98,0.81 DT 0.8 0.88 0.93
val pre_Data=hiveCtx.sql("select KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,SEARCH_BJ_FLAG,SEARCH_ZJ_FLAG,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,cluster2,cluster3,if(ARPU<50,1,0) low_arpu,if(ARPU>=50 and ARPU<150,1,0) mid_arpu,if(ARPU>=150,1,0) high_arpu,if(use_num<500,1,0) low_gprs,if(use_num>=500 and use_num<2000,1,0) mid_gprs,if(use_num>=2000,1,0) high_gprs,new_user_flag,LABEL from jcc_jh_dataset_16 ");

val data1=pre_Data.rdd.map(x =>(x(0).toString.toDouble, x(1).toString.toDouble,x(2).toString.toDouble,x(3).toString.toDouble,x(4).toString.toDouble, x(5).toString.toDouble,x(6).toString.toDouble,x(7).toString.toDouble,x(8).toString.toDouble,x(9).toString.toDouble,x(10).toString.toDouble,x(11).toString.toDouble,x(12).toString.toDouble,x(13).toString.toDouble,x(14).toString.toDouble,x(15).toString.toDouble,x(16).toString.toDouble,x(17).toString.toDouble,x(18).toString.toDouble));

val data2=data1.map(x=>LabeledPoint(x._19,Vectors.dense(x._1,x._2,x._3,x._4,x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14,x._15,x._16,x._17,x._18)));


//去掉SEARCH_BJ_FLAG,SEARCH_ZJ_FLAG RG 0.63 0.98 0.81 DT 0.8 0.88 0.93 BFGS 0.7 0.82 0.84
val pre_Data=hiveCtx.sql("select KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,cluster2,cluster3,if(ARPU<50,1,0) low_arpu,if(ARPU>=50 and ARPU<150,1,0) mid_arpu,if(ARPU>=150,1,0) high_arpu,if(use_num<500,1,0) low_gprs,if(use_num>=500 and use_num<2000,1,0) mid_gprs,if(use_num>=2000,1,0) high_gprs,new_user_flag,LABEL from jcc_jh_dataset_16 ");

val data1=pre_Data.rdd.map(x =>(x(0).toString.toDouble, x(1).toString.toDouble,x(2).toString.toDouble,x(3).toString.toDouble,x(4).toString.toDouble, x(5).toString.toDouble,x(6).toString.toDouble,x(7).toString.toDouble,x(8).toString.toDouble,x(9).toString.toDouble,x(10).toString.toDouble,x(11).toString.toDouble,x(12).toString.toDouble,x(13).toString.toDouble,x(14).toString.toDouble,x(15).toString.toDouble,x(16).toString.toDouble));

val data2=data1.map(x=>LabeledPoint(x._17,Vectors.dense(x._1,x._2,x._3,x._4,x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14,x._15,x._16)));


//增加是否有套外流量overflow RG 0.48 ,0.99,0.74 DT 0.8 0.88 0.93
val pre_Data=hiveCtx.sql("select KD_RATE,AGE_RATE,CMCC_KD_CNT,HOME_DEPEND,AGE_FLAG1,AGE_FLAG2,age_flag3,age_flag4,cluster2,cluster3,if(ARPU<50,1,0) low_arpu,if(ARPU>=50 and ARPU<150,1,0) mid_arpu,if(ARPU>=150,1,0) high_arpu,if(use_num<500,1,0) low_gprs,if(use_num>=500 and use_num<2000,1,0) mid_gprs,if(use_num>=2000,1,0) high_gprs,if(use_num2>=0,1,0) overflow,new_user_flag,LABEL from jcc_jh_dataset_16 ");

val data1=pre_Data.rdd.map(x =>(x(0).toString.toDouble, x(1).toString.toDouble,x(2).toString.toDouble,x(3).toString.toDouble,x(4).toString.toDouble, x(5).toString.toDouble,x(6).toString.toDouble,x(7).toString.toDouble,x(8).toString.toDouble,x(9).toString.toDouble,x(10).toString.toDouble,x(11).toString.toDouble,x(12).toString.toDouble,x(13).toString.toDouble,x(14).toString.toDouble,x(15).toString.toDouble,x(16).toString.toDouble,x(17).toString.toDouble));

val data2=data1.map(x=>LabeledPoint(x._18,Vectors.dense(x._1,x._2,x._3,x._4,x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14,x._15,x._16,x._17)));




val splits = data2.randomSplit(Array(0.7, 0.3), seed = 11L);

val (trainingData, testData) = (splits(0).cache(), splits(1).cache());


val numIterations = 100;
 
val model_log = LogisticRegressionWithSGD.train(trainingData, numIterations);

println("weight:%s, intercept:%s ".format(model_log.weights,model_log.intercept));



val log_Preds = testData.map { x =>
  val pred = model_log.predict(x.features)
  (pred, x.label)
}

val accuracy_log = 1.0 * log_Preds.filter(x => x._1 == x._2).count() / testData.count()

val log_r = 1.0 * log_Preds.filter(x => (x._1 == 1.0) && (x._1 == x._2) ).count() / log_Preds.filter(x => x._2 == 1.0 ).count()
val log_p = 1.0 * log_Preds.filter(x => (x._1 == 1.0) && (x._1 == x._2) ).count() / log_Preds.filter(x => x._1 == 1.0 ).count()

val metrics_log = new BinaryClassificationMetrics(log_Preds)
val auROC_log = metrics_log.areaUnderROC()

println("Area under ROC = " + auROC_log)


//DT


  If (feature 15 <= 0.0)
   If (feature 16 <= 0.0)
    Predict: 1.0
   Else (feature 16 > 0.0)
    Predict: 0.0
  Else (feature 15 > 0.0)
   Predict: 0.0


val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 8
val maxBins = 512
val modelDT = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins)
  
  val DT_Preds = testData.map { point =>
  val prediction = modelDT.predict(point.features)
  (point.label, prediction)
}


val accuracy_DT = 1.0 * DT_Preds.filter(x => x._1 == x._2).count() / testData.count() //0.88
val DT_r = 1.0 * DT_Preds.filter(x => (x._1 == 1.0) && (x._1 == x._2) ).count() / DT_Preds.filter(x => x._1 == 1.0 ).count()
val DT_p = 1.0 * DT_Preds.filter(x => (x._1 == 1.0) && (x._1 == x._2) ).count() / DT_Preds.filter(x => x._2 == 1.0 ).count()

val metrics_DT = new BinaryClassificationMetrics(DT_Preds)
val auROC_DT = metrics_DT.areaUnderROC()

println("Learned classification tree model:\n" + modelDT.toDebugString)